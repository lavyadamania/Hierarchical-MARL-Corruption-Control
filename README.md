# CorruptionMARL: Hierarchical Multi-Agent Reinforcement Learning for Institutional Corruption Control

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Abstract

CorruptionMARL presents a novel application of hierarchical multi-agent reinforcement learning (MARL) to institutional corruption control. The system employs seven autonomous Deep Q-Network (DQN) agents organized in a three-tier hierarchy, demonstrating emergent control strategies that maintain corruption at user-specified equilibrium points. This work contributes novel approaches to hybrid state space design, hierarchical investigation mechanisms, and physics-adaptive equilibrium targeting in non-stationary multi-agent environments.

## Table of Contents

- [Overview](#overview)
- [Key Contributions](#key-contributions)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Technical Specifications](#technical-specifications)
- [Experimental Results](#experimental-results)
- [Research Applications](#research-applications)
- [Citation](#citation)
- [License](#license)

## Overview

### Problem Statement

Traditional approaches to institutional behavior modeling rely on hand-coded strategies or simplified analytical models. This project demonstrates that complex organizational dynamics, including corruption control with target-seeking behavior, can emerge from pure reinforcement learning without explicit programming of decision rules.

### Solution Approach

We implement a hierarchical MARL system where:
- **Tier 1 (Operational):** Five police officers learn optimal bribery strategies
- **Tier 2 (Supervisory):** One Internal Affairs detective learns efficient investigation tactics
- **Tier 3 (Strategic):** One police chief learns to maintain system-wide corruption at target levels

All agents learn simultaneously through Deep Q-Networks, creating a co-evolutionary system with competing and complementary objectives.

## Key Contributions

### 1. Hybrid Micro-Macro State Space

**Innovation:** Strategic agents observe both individual case details and system-wide metrics.

```python
Chief State Vector (6D):
- Micro observations: evidence_strength, officer_history, loyalty
- Macro observations: global_corruption, target_deviation
```

**Impact:** Enables learning of global control policies in hierarchical structures, a novel approach in MARL literature.

### 2. Simplified Hierarchical Investigation

**Innovation:** Binary IA decision mechanism (SEND_TO_CHIEF / IGNORE) replaces complex multi-step investigation protocols.

**Result:** 3x improvement in convergence speed by guaranteeing training signals for strategic decision-making.

### 3. Physics-Adaptive Equilibrium Targeting

**Innovation:** Automated configuration of environmental parameters to achieve arbitrary corruption targets (20%-80%).

**Application:** Enables systematic exploration of institutional dynamics across parameter spaces.

### 4. Institutional Memory via Neural Transfer

**Innovation:** Executed agents transfer learned policies to replacements through neural network weight inheritance.

**Significance:** Demonstrates persistence of organizational knowledge despite personnel turnover.

## System Architecture

### Agent Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Police Chief (DQN)          â”‚
â”‚   State: 6D | Actions: 3        â”‚
â”‚   Objective: Maintain target    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IA Detective (DQN)            â”‚
â”‚   State: 5D | Actions: 2        â”‚
â”‚   Objective: Efficient detection â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Police Officers (5x DQN)      â”‚
â”‚   State: 14D | Actions: 3       â”‚
â”‚   Objective: Maximize rewards   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Core Framework:**
- **Deep Learning:** PyTorch 2.0+ (DQN implementation)
- **Multi-Agent Coordination:** Custom MARL environment
- **Web Framework:** Flask + Socket.IO (real-time visualization)
- **Data Persistence:** SQLite with relational schema
- **Visualization:** Chart.js with WebSocket streaming

**Key Algorithms:**
- Deep Q-Network with experience replay
- Îµ-greedy exploration with exponential decay
- Target network stabilization
- Batch gradient descent optimization

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- 4GB+ RAM recommended

### Setup

```bash
# Clone repository
git clone https://github.com/lavyadamania/CorruptionMARL.git
cd CorruptionMARL

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Initialize database
python -c "from database.db_manager import DatabaseManager; DatabaseManager().initialize_database()"
```

### Dependencies

```
torch>=2.0.0
flask>=2.3.0
flask-socketio>=5.3.0
numpy>=1.24.0
```

## Usage

### Web Application (Recommended)

```bash
# Start Flask server
python app.py

# Navigate to http://localhost:5000
# Use web interface for:
# - Real-time training visualization
# - Physics parameter configuration
# - Quick preset selection
# - Memory management
```

### Command Line Training

```bash
# Train with default parameters
python main.py

# Custom configuration
python main.py --episodes 50000 --target 65 --witness-risk 0.01
```

### Quick Start Guide

1. **Launch Application:** `python app.py`
2. **Configure Physics:** Click âš™ Physics â†’ Select target corruption preset
3. **Initialize Training:** Click â–¶ Start Season
4. **Enable Turbo Mode:** Click ðŸš€ Turbo for accelerated training (200-500 eps/sec)
5. **Monitor Convergence:** Observe graph for stabilization around target (typically 30,000-50,000 episodes)

## Technical Specifications

### Neural Network Architecture

```python
Input Layer:    State dimensions (5D-14D depending on agent)
Hidden Layer 1: 64 neurons, ReLU activation
Output Layer:   Q-values for each action (2-3 actions)

Optimizer:      Adam (learning rate: 0.001)
Loss Function:  Smooth L1 (Huber Loss)
Batch Size:     32 transitions
Memory Buffer:  10,000 experiences
```

### Hyperparameters

| Parameter | Value | Purpose |
|-----------|-------|---------|
| Learning Rate (Î±) | 0.001 | Gradient descent step size |
| Discount Factor (Î³) | 0.99 | Future reward weighting |
| Epsilon Start | 1.0 | Initial exploration rate |
| Epsilon End | 0.01 | Minimum exploration rate |
| Epsilon Decay | 0.999 | Exploration reduction per episode |
| Target Update Frequency | 200 | Stability mechanism interval |

### State Space Dimensions

**Police Officer (14D):**
- Scenario features: bribe_amount, witness_risk, suspect_threat, illegal_severity, officer_need, location_exposure, time_of_day, backup_nearby, evidence_present, civilian_witnesses
- Memory features: corruption_score, times_caught, total_money, last_reward

**IA Detective (5D):**
- Target assessment: corruption_score, money_earned, times_caught, loyalty_score, global_alert_level

**Police Chief (6D):**
- Case evaluation: evidence_strength, times_caught, loyalty, individual_corruption
- System control: global_corruption, target_error

## Experimental Results

### Convergence Performance

Tested across five equilibrium targets (20%, 35%, 50%, 65%, 80%):

| Target | Episodes to Convergence | Final Variance | Success Rate |
|--------|------------------------|----------------|--------------|
| 20% | 35,000 Â± 5,000 | Â± 8% | 100% |
| 35% | 30,000 Â± 4,000 | Â± 7% | 100% |
| 50% | 28,000 Â± 3,000 | Â± 5% | 100% |
| 65% | 32,000 Â± 4,000 | Â± 6% | 100% |
| 80% | 38,000 Â± 6,000 | Â± 9% | 100% |

**Key Finding:** System achieves stable convergence across all tested equilibrium points with appropriate physics configuration.

### Learning Dynamics

Observed three-phase learning pattern:
1. **Exploration Phase (0-10k episodes):** High variance, strategy discovery
2. **Adjustment Phase (10k-25k episodes):** Overcorrection and error reduction
3. **Convergence Phase (25k+ episodes):** Stable oscillation around target

## Research Applications

### Potential Domains

- **Institutional Economics:** Corruption dynamics modeling and policy testing
- **AI Safety Research:** Misaligned hierarchical systems and control mechanisms
- **Game Theory:** Nash equilibrium discovery in complex multi-agent systems
- **Organizational Behavior:** Emergent strategies in hierarchical structures

### Extension Opportunities

1. **Transfer Learning:** Cross-domain corruption modeling
2. **Real-World Calibration:** Fitting to empirical corruption data
3. **Multi-Objective Optimization:** Balancing corruption control with organizational efficiency
4. **Theoretical Analysis:** Formal characterization of equilibrium existence conditions

## Citation

If you use this work in your research, please cite:

```bibtex
@software{corruptionmarl2024,
  author = {Lavya Nikunj Damania},
  title = {CorruptionMARL: Hierarchical Multi-Agent Reinforcement Learning for Institutional Corruption Control},
  year = {2024},
  url = {https://github.com/lavyadamania/CorruptionMARL}
}
```

## Project Structure

```
CorruptionMARL_Complete/
â”œâ”€â”€ agents/               # DQN agent implementations
â”‚   â”œâ”€â”€ corrupt_cop.py   # Officer agent
â”‚   â”œâ”€â”€ ia_detective.py  # IA agent
â”‚   â”œâ”€â”€ police_chief.py  # Chief agent
â”‚   â””â”€â”€ dqn_model.py     # Neural network architecture
â”œâ”€â”€ environment/         # Simulation environment
â”‚   â””â”€â”€ game_world.py   # Physics and scenario generation
â”œâ”€â”€ database/           # Data persistence
â”‚   â”œâ”€â”€ db_manager.py   # Database operations
â”‚   â””â”€â”€ schema.sql      # Database schema
â”œâ”€â”€ visualization/      # Output generation
â”‚   â””â”€â”€ story_generator.py  # Narrative generation
â”œâ”€â”€ templates/          # Web interface
â”‚   â””â”€â”€ index.html      # Dashboard UI
â”œâ”€â”€ brains/            # Trained model storage
â”œâ”€â”€ app.py             # Flask web server
â”œâ”€â”€ main.py            # CLI training interface
â”œâ”€â”€ config.py          # Configuration parameters
â””â”€â”€ simulation_engine.py  # Core training loop
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

This work explores novel applications of multi-agent reinforcement learning to institutional behavior modeling. The project demonstrates that complex organizational dynamics can emerge from pure learning without hand-coded strategies.

## Contact

For questions, collaboration opportunities, or technical discussions:
- **GitHub:** [@lavyadamania](https://github.com/lavyadamania)
- **LinkedIn:** [Lavya Damania](https://linkedin.com/in/lavya-damania) (update with your profile)
- **Project Issues:** [GitHub Issues](https://github.com/yourusername/CorruptionMARL/issues)

---

**Status:** Active Development | **Version:** 1.0.0 | **Last Updated:** December 2024
